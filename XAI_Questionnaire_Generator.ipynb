{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b105c191-06db-4b89-a767-93eebd2ff4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os\n",
    "\n",
    "# Installation of the required libraries and retrieving of the needed dependencies if the notebook runs in Google Colaboratory\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # Libraries installation\n",
    "    os.system(\"pip install ipywidgets --quiet\")\n",
    "    os.system(\"pip install nbformat --quiet\")\n",
    "    \n",
    "    # Download required files from Github repository\n",
    "    os.system(\"wget https://github.com/marcozenere/XAI_Survey_Generator/archive/refs/heads/main.zip\")\n",
    "    os.system(\"unzip /content/main.zip \")\n",
    "\n",
    "    # Get some of the repository files and remove the directory\n",
    "    os.system(\"mv ./XAI_Survey_Generator-main/functions.py ./functions.py\")\n",
    "    os.system(\"mv ./XAI_Survey_Generator-main/Images ./Images/\")\n",
    "    os.system(\"rm -r XAI_Survey_Generator-main\")\n",
    "    os.system(\"rm main.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6f8126-7f42-43d6-9f85-99fa0ba6e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5eb449-306c-4591-94fd-73dd1ae6d236",
   "metadata": {},
   "source": [
    "# XAI Questionnaire Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06200be9-b0a8-475f-9766-732868644c4c",
   "metadata": {},
   "source": [
    "The following notebook has been created with the aim of generating a template of a questionnaire suitable for making evaluation in the XAI field. \n",
    "\n",
    "In order to generate a survey template suited for the user's needs, the following program requires you to answer the questions below. Later, after clicking the button 'Generate Template', the program will save the template in a file with the .ipynb extension that contains, in addition to the survey template, all the information necessary to deploy it.\n",
    "\n",
    "If you run this notebook locally, the files generated by the program will be located in a directory called \"XAI_Questionnaire\", situated in the directory where you run the notebook. If you run the notebook in Google Colaboratory, the program will generate a zip file and download it to your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbfd81-b4fc-4027-b0ba-a094f71e62fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7cd65-f01e-4567-a94a-b00216dc2010",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 1\n",
    "\n",
    "#### Qualitative Evaluation \n",
    "* A qualitative evaluation typically consists of open-ended questions that gather no numerical data to achieve deeper insight into linguistic choices made by the system. \n",
    "\n",
    "#### Quantitative Evaluation\n",
    "* A qualitative evaluation, often viewed as scientifically objective and rational, typically consists of close-ended questions which gather numerical data. It is most well suited for testing hypotheses and statistical analysis.\n",
    "\n",
    "[[Further Information at this link]](https://reader.elsevier.com/reader/sd/pii/S1566253521001093?token=7EAC2E0FDCC8CDE2120382AA3CFB060604140EFEE9F3CBB9761BBC2BB2A716E830D40CE2AF0C1BF55E60B52C5756CDA9&originRegion=eu-west-1&originCreation=202205021658280)\n",
    "\n",
    "[[Further Information at this link]](https://www.researchgate.net/publication/347355146_Human_evaluation_of_automatically_generated_text_Current_trends_and_best_practice_guidelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b8220-7e76-4faa-8a1b-c5e1aa11721e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Considering the provided definitions, what type of evaluation you would like to do in your questionnaire?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ceb4682-ce15-4b51-abfa-14c431b56886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c974d6d09cd9405ebcc12e3f4aeb1b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(options=(('Qualitative Evaluation', '1'), ('Quantitative Evaluation', '2')), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(question1RadioButtons())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee956d96-0ea5-4922-ae48-4053fca6ac5d",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b913e-b225-4993-b1a7-1b43b8e48ba2",
   "metadata": {},
   "source": [
    "The evaluation that we normally would like to make when carrying out a questionnaire in the XAI field, is to evaluate whether the explanation generated by our XAI system satisfies one or more of the following goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2a1ac8-2420-4ac9-a020-815a3dadcb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfbafc6f2cf4caaaf80807bb0f6dbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x04\\xaa\\x00\\x00\\x02\\x1e\\x08\\x06\\x00\\x00\\x00\\x9bk\\xe7…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imageLoader(\"./Images/Explanation_Goals.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca34386-df55-466f-be6f-8f9d430fbdbd",
   "metadata": {},
   "source": [
    "To test the hypothesis, it is necessary to use a methodology. Four different methodologies can be used for this purpose, and each differs in the evaluation method and information provided to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da54d2c-c036-4bd6-ac51-bdf8c9129364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da812c2f67442528e2149a96ccbfce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x05\\x8a\\x00\\x00\\x02;\\x08\\x06\\x00\\x00\\x00B\\xd9\\xaf\\xd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imageLoader(\"./Images/Methodology_Assessment_Explanation_Quality.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fd3f7-d9d2-4ad5-9d05-6b254200ddf2",
   "metadata": {},
   "source": [
    "* In Verification task, participants are provided with an input, output, and explanation to ask them about their satisfaction with the explanation.\n",
    "\n",
    "* In Forced Choice tasks, participants are provided with an input, output, and multiple explanations to ask them which one is the most suited.\n",
    "\n",
    "* In Forward Simulation tasks, participants are provided with an input and an explanation to ask them to predict the system's output.\n",
    "\n",
    "* In Counterfactual Simulation tasks, participants are provided with an input, output, alternative output (counterfactual) and an explanation to ask them to predict the input changes to obtain the alternative output.\n",
    "\n",
    "[[Further Information at this link]](http://ceur-ws.org/Vol-2582/paper9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2a178-d166-4df0-8fb8-d7c0bf3987a8",
   "metadata": {},
   "source": [
    "Considering the provided information, which methodology would you like to use to test the explanation goal/goals you fixed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347e6319-0850-4cbb-ace1-4902b8d927b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8333f658339f43fb8dff163760339dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(options=(('Verification', '1'), ('Forced Choice', '2'), ('Forward Simulation', '3'), ('Counterfac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explanation_assessment_quality = question2RadioButtons()\n",
    "explanation_assessment_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f07d28-3bcd-441b-9d19-e603770932db",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8a5ff-80d7-4771-91eb-a92e47955057",
   "metadata": {},
   "source": [
    "How many questions would you like to have in your questionnaire?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b350b5e9-c913-482c-9c5f-0c50f96231cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3f1bced7f94cb3a831cddaad39986a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions_number = question3IntText()\n",
    "questions_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "180c7902-7915-4291-9075-215af86f20a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03eac70fd368466b8b941a787b02e08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Generate Template', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd762e569b14ecba09e56831baeff13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_button = generateButton()\n",
    "output = widgets.Output()\n",
    "generate_button.on_click(functools.partial(generate_button_clicked, output = output, intepreter_type = get_ipython()))\n",
    "display(generate_button,output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
