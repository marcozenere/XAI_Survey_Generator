{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b105c191-06db-4b89-a767-93eebd2ff4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os\n",
    "\n",
    "# Installation of the required libraries and retrieving of the needed dependencies if the notebook runs in Google Colaboratory\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # Libraries installation\n",
    "    os.system(\"pip install ipywidgets --quiet\")\n",
    "    os.system(\"pip install nbformat --quiet\")\n",
    "    \n",
    "    # Download required files from Github repository\n",
    "    os.system(\"wget https://github.com/marcozenere/XAI_Survey_Generator/archive/refs/heads/main.zip\")\n",
    "    os.system(\"unzip /content/main.zip \")\n",
    "\n",
    "    # Get some of the repository files and remove the directory\n",
    "    os.system(\"mv ./XAI_Survey_Generator-main/functions.py ./functions.py\")\n",
    "    os.system(\"mv ./XAI_Survey_Generator-main/Images ./Images/\")\n",
    "    os.system(\"rm -r XAI_Survey_Generator-main\")\n",
    "    os.system(\"rm main.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6f8126-7f42-43d6-9f85-99fa0ba6e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5eb449-306c-4591-94fd-73dd1ae6d236",
   "metadata": {},
   "source": [
    "# XAI Questionnaire Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06200be9-b0a8-475f-9766-732868644c4c",
   "metadata": {},
   "source": [
    "The following notebook has been created with the aim of generating a template of a questionnaire suitable for making evaluation in the XAI field. \n",
    "\n",
    "In order to generate a survey template suited for the user's needs, the following program requires you to answer the questions below. Later, after clicking the button 'Generate Template', the program will save the template in a file with the .ipynb extension that contains, in addition to the survey template, all the information necessary to deploy it.\n",
    "\n",
    "If you run this notebook locally, the files generated by the program will be located in a directory called \"XAI_Questionnaire\", situated in the directory where you run the notebook. If you run the notebook in Google Colaboratory, the program will generate a zip file and download it to your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbfd81-b4fc-4027-b0ba-a094f71e62fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7cd65-f01e-4567-a94a-b00216dc2010",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 1\n",
    "\n",
    "#### Qualitative Evaluation \n",
    "* A qualitative evaluation typically consists of open-ended questions that gather no numerical data to achieve deeper insight into linguistic choices made by the system. \n",
    "\n",
    "#### Quantitative Evaluation\n",
    "* A quantitative evaluation, often viewed as scientifically objective and rational, typically consists of close-ended questions which gather numerical data. It is most well suited for testing hypotheses and statistical analysis.\n",
    "\n",
    "If you need futher details, please read:\n",
    "* [Notions of explainability and evaluation approaches for explainable artificial intelligence](https://www.sciencedirect.com/science/article/pii/S1566253521001093?via%3Dihub)\n",
    "* [Human evaluation of automatically generated text: Current trends and best practice guidelines](https://www.sciencedirect.com/science/article/pii/S088523082030084X?via%3Dihub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b8220-7e76-4faa-8a1b-c5e1aa11721e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Considering the definitions provided, select the type of evaluation you would like to do in your XAI evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ceb4682-ce15-4b51-abfa-14c431b56886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fb5446bde4497ca5457405d481724d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(options=(('Qualitative Evaluation', '1'), ('Quantitative Evaluation', '2')), value=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(question1RadioButtons())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee956d96-0ea5-4922-ae48-4053fca6ac5d",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b913e-b225-4993-b1a7-1b43b8e48ba2",
   "metadata": {},
   "source": [
    "In the context of XAI evaluation, you are likely to be interested in finding out whether automated explanations satisfy one or more of the following goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2a1ac8-2420-4ac9-a020-815a3dadcb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064530df90cc44a3afbe231061f9ac2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x04\\xaa\\x00\\x00\\x02\\x1e\\x08\\x06\\x00\\x00\\x00\\x9bk\\xe7…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imageLoader(\"./Images/Explanation_Goals.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca34386-df55-466f-be6f-8f9d430fbdbd",
   "metadata": {},
   "source": [
    "The selection of one or more explanation goals, after choosing the hypothesis to be empirically validated with your XAI evaluation, defines the type of task to carry out the desired empirical validation. Each task is aimed for a different goal, uses a specific evaluation method and provides survey participants with different information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da54d2c-c036-4bd6-ac51-bdf8c9129364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3540131e4bdd40558d84182b78993336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x05\\x8a\\x00\\x00\\x02;\\x08\\x06\\x00\\x00\\x00B\\xd9\\xaf\\xd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imageLoader(\"./Images/Methodology_Assessment_Explanation_Quality.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fd3f7-d9d2-4ad5-9d05-6b254200ddf2",
   "metadata": {},
   "source": [
    "* In Verification tasks, participants are provided with an input, output, and explanation to ask them about their satisfaction with the explanation.\n",
    "\n",
    "* In Forced Choice tasks, participants are provided with an input, output, and multiple explanations to ask them which one is the most suited.\n",
    "\n",
    "* In Forward Simulation tasks, participants are provided with an input and an explanation to ask them to predict the system's output.\n",
    "\n",
    "* In Counterfactual Simulation tasks, participants are provided with an input, output, alternative output (counterfactual) and an explanation to ask them to predict the input changes to obtain the alternative output.\n",
    "\n",
    "If you need futher details, please read:\n",
    "* [A Taxonomy for Human Subject Evaluation of Black-box Explanations in XAI](http://ceur-ws.org/Vol-2582/paper9.pdf): A proposal of a taxonomy that provides guidance for researchers and practitioners on the design and execution of XAI evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f2a178-d166-4df0-8fb8-d7c0bf3987a8",
   "metadata": {},
   "source": [
    "In the current version of the software, the correlation between the explanation goals and the task types is not implemented yet.\n",
    "For the master thesis, the selection is forced to \"Transparency\" as the explanation goal and \"Forward simulation\" as the task type. The selection is based on the desired empirical evaluation we would like to carry out in the XAI Survey available at this [link](https://github.com/marcozenere/XAI_Survey)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f07d28-3bcd-441b-9d19-e603770932db",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8a5ff-80d7-4771-91eb-a92e47955057",
   "metadata": {},
   "source": [
    "How many questions would you like to have in your questionnaire?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b350b5e9-c913-482c-9c5f-0c50f96231cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215887d9d6014ba98bda36b7ed0a0266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions_number = question3IntText()\n",
    "questions_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "180c7902-7915-4291-9075-215af86f20a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9905a85a55aa4fadb6827444d103fafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Generate Template', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f617b669f96a429a9f559e462a21c41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_button = generateButton()\n",
    "output = widgets.Output()\n",
    "generate_button.on_click(functools.partial(generate_button_clicked, output = output, intepreter_type = get_ipython()))\n",
    "display(generate_button,output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
